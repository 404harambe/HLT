{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we experiment with sentiment analysis and POS tagginig by implementing a number of simple classifiers based on Nayve Bayes and Neural Networks. This is intended as a starting point only and no model selection or other fancy stuff is done.\n",
    "\n",
    "# Naive Bayes classifier (IMDB dataset)\n",
    "In this part we create a naive bayes classifier for the IMDB movie reviews dataset. Make sure you are able to import all the necessary dependencies. The smoothing term is the parameter of the Laplacian smoothing, to avoid zero-ing probabilities for documents with unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "SMOOTHING_TERM = 1 #Smoothing term to avoid trouble with 0 probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the IMDB dataset, as imported from keras, encodes words with numbers, we define a function to decode the reviews i.e. get a readable version. The UNK token is given by the preprocessing already applied in the imported dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Facility to decode reviews (i.e. give it an element from x_train or x_test as\n",
    "#loaded from imdb.load_data, and it will return the readable review.\n",
    "def decode_review(review):\n",
    "    word_to_id = keras.datasets.imdb.get_word_index()\n",
    "    word_to_id = {k: (v + 3) for k, v in word_to_id.items()}\n",
    "    word_to_id[\"<PAD>\"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2\n",
    "    id_to_word = {value: key for key, value in word_to_id.items()}\n",
    "    return ' '.join(id_to_word[id] for id in review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the dataset and count the occurences of each class $c_i$ (0 for negative review and 1 for positive, in this case). From that we can determine the probability $P(c_i)$ for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5, 1: 0.5}\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "#Count the occurences of each class\n",
    "occ_classes = Counter(y_train)\n",
    "\n",
    "instances = len(y_train)\n",
    "#Calculate the probability of each class (i.e. occurences/(total number of instances))\n",
    "p_classes = {key: value/instances for key,value in occ_classes.items()}\n",
    "print(p_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now separate the positive reviews from the negative ones, and count the occurences of each word $w_i$ in positive and negative contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate positive reviews from negative ones\n",
    "x_train_pos, x_train_neg = [], []\n",
    "for i in range(0, instances):\n",
    "    (x_train_pos, x_train_neg)[1 if y_train[i]==0 else 0].append(x_train[i])\n",
    "\n",
    "#Count the occurences of each word\n",
    "w_occ = {key: value for key,value in\n",
    "     Counter(x for xs in x_train for x in xs).items()}\n",
    "\n",
    "#Count the number of occurences of each word in positive reviews\n",
    "w_c_pos = {key: value for key,value in\n",
    "     Counter(x for xs in x_train_pos for x in xs).items()}\n",
    "\n",
    "#Count the number of occurences of each word in negative reviews\n",
    "w_c_neg = {key: value for key,value in\n",
    "     Counter(x for xs in x_train_neg for x in xs).items()}\n",
    "\n",
    "#Add the words with 0 occurences to both dictionaries\n",
    "for key in w_occ.keys():\n",
    "    if key not in w_c_pos.keys():\n",
    "        w_c_pos[key] = 0\n",
    "    if key not in w_c_neg.keys():\n",
    "        w_c_neg[key] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the conditional probabilities $P(w_i|c_j)$, that is the probability of each word of appearing in a positive review and the probability of each word of appearing in a negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the probability of each word in of appearing in a negative\n",
    "#review (i.e. P(w|p_classes['0']) for each w in the vocabulary)\n",
    "w_p_pos = {key:(value+SMOOTHING_TERM)/(w_occ[key]+2*SMOOTHING_TERM)\n",
    "           for key, value in w_c_pos.items()}\n",
    "\n",
    "#Calculate the probability of each word in of appearing in a negative\n",
    "#review (i.e. P(w|p_classes['0']) for each w in the vocabulary)\n",
    "w_p_neg = {key:(value+SMOOTHING_TERM)/(w_occ[key]+2*SMOOTHING_TERM)\n",
    "           for key, value in w_c_neg.items()}\n",
    "\n",
    "#Safety check that all probabilities (p(w|pos)+p(w|neg)) indeed sum\n",
    "#up to 1\n",
    "for key in w_occ.keys():\n",
    "    if (1-((w_p_pos[key] if key in w_p_pos.keys() else 0)+\n",
    "        (w_p_neg[key] if key in w_p_neg.keys() else 0))>1e-6):\n",
    "            exit('Something\\'s wrong')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is now complete: given a review $r$, its class $c_r$ is given by $c_r=\\underset{c_j\\in C}{\\operatorname{argmax}} log(P(c_j))+\\sum_{i \\in values} P(x_i|c_j)$. We obtain the accuracy, precision, recall and F1-measure on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.84408\n",
      "Training set precision: 0.8084923253478697\n",
      "Training set recall: 0.90176\n",
      "Training set F1-measure: 0.8525830118750474\n"
     ]
    }
   ],
   "source": [
    "t_p = 0; t_n = 0; f_p = 0; f_n = 0\n",
    "for el in range(0,len(x_train)):\n",
    "    ppos = 0\n",
    "    pneg = 0\n",
    "    for x in x_train[el]:\n",
    "        ppos = ppos + np.log(w_p_pos[x])\n",
    "        pneg = pneg + np.log(w_p_neg[x])\n",
    "    if (((ppos>=pneg)) and y_train[el]==1):\n",
    "        t_p+=1\n",
    "    elif (((ppos>=pneg)) and y_train[el]==0):\n",
    "        f_p+=1\n",
    "    elif (((ppos<pneg)) and y_train[el]==1):\n",
    "        f_n+=1\n",
    "    elif (((ppos<pneg)) and y_train[el]==0):\n",
    "        t_n+=1\n",
    "\n",
    "print('Training set accuracy:',(t_n+t_p)/(t_p+t_n+f_p+f_n))\n",
    "print('Training set precision:',(t_p)/(t_p+f_p))\n",
    "print('Training set recall:',(t_p)/(t_p+f_n))\n",
    "print('Training set F1-measure:',2*(((t_p)/(t_p+f_p))*((t_p)/(t_p+f_n))/(((t_p)/(t_p+f_p))+((t_p)/(t_p+f_n)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the accuracy is relatively low. Could probably be increased with more meaningful features, e.g. the length of the review. We can calculate the same measures on the test set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.82204\n",
      "Test set precision: 0.795189557820635\n",
      "Test set recall: 0.86752\n",
      "Test set F1-measure: 0.829781535753912\n"
     ]
    }
   ],
   "source": [
    "t_p = 0; t_n = 0; f_p = 0; f_n = 0\n",
    "for el in range(0,len(x_test)):\n",
    "    ppos = 0\n",
    "    pneg = 0\n",
    "    for x in x_test[el]:\n",
    "        ppos = ppos + np.log(w_p_pos[x])\n",
    "        pneg = pneg + np.log(w_p_neg[x])\n",
    "    if (((ppos>=pneg)) and y_test[el]==1):\n",
    "        t_p+=1\n",
    "    elif (((ppos>=pneg)) and y_test[el]==0):\n",
    "        f_p+=1\n",
    "    elif (((ppos<pneg)) and y_test[el]==1):\n",
    "        f_n+=1\n",
    "    elif (((ppos<pneg)) and y_test[el]==0):\n",
    "        t_n+=1\n",
    "\n",
    "print('Test set accuracy:',(t_n+t_p)/(t_p+t_n+f_p+f_n))\n",
    "print('Test set precision:',(t_p)/(t_p+f_p))\n",
    "print('Test set recall:',(t_p)/(t_p+f_n))\n",
    "print('Test set F1-measure:',2*(((t_p)/(t_p+f_p))*((t_p)/(t_p+f_n))/(((t_p)/(t_p+f_p))+((t_p)/(t_p+f_n)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network classifier\n",
    "In this part we create a neural network classifier for the IMDB movie reviews dataset. Make sure you are able to import all the necessary dependencies. The architecture of the network is as follows:\n",
    "- Embedding layer: maps input words from the vocabulary to lower dimension embeddings;\n",
    "- 1D convolution layer: to treat the sequences of embeddings constituing each review;\n",
    "- Max pooling layer: to add some sort of invariance to small translations (probably not a great idea in this case);\n",
    "- Dense layers to perform the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence: 2494 \n",
      "Average sequence length: 238.71364 \n",
      "Sequences length std: 176.49367364852034 \n",
      "Max sequence length to consider: 503 \n",
      "Vocabulary size 88584\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.layers import ZeroPadding1D, Convolution1D,MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "#Reload just for clarity\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "#Length of longest sequence in the dataset\n",
    "max_len = np.max([len(el) for el in x_train])\n",
    "\n",
    "#Average length of sequences in the dataset\n",
    "avg_len = np.average([len(el) for el in x_train])\n",
    "\n",
    "#Std deviation of sequences' length in the dataset\n",
    "std_len = np.std([len(el) for el in x_train])\n",
    "\n",
    "#The maximum length sequence we will consider\n",
    "seq_len = int(avg_len+std_len*1.5)\n",
    "\n",
    "#The size of the vocabulary\n",
    "vocab_size = len(imdb.get_word_index())\n",
    "\n",
    "\n",
    "print(\"Longest sequence:\",max_len,\"\\nAverage sequence length:\",avg_len,\"\\nSequences length std:\",std_len,\n",
    "      \"\\nMax sequence length to consider:\",seq_len,\"\\nVocabulary size\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a maximum sequence length allowed: choosing this as the length of the longest sequence would result in a huge memory consumption due to the padding for the rest of the sequences. We therefore take the average length of the sequence + $\\alpha\\cdot$(the standard deviation). Sequences longer than that will be trimmed, whilst shorter ones will be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 14s 560us/step - loss: 0.5094 - acc: 0.7037\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 13s 540us/step - loss: 0.2261 - acc: 0.9142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82c9896080>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_p = pad_sequences(x_train, maxlen=seq_len, padding='post')\n",
    "x_test_p = pad_sequences(x_test, maxlen=seq_len, padding='post')\n",
    "\n",
    "#Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=seq_len))\n",
    "model.add(Convolution1D(10, 5, activation='relu')),\n",
    "model.add(MaxPooling1D()),\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation='relu')),\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Compile and fit the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train_p, y_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 172us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3136934424877167, 0.87324]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_p,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, even with such a small model, the neural network approach leads to better results then the bayes one. The difference in accuracy between the training and test set may indicate some overfitting - which would not be suprising as no measures to prevent it (e.g. Dropout layers, or norm penalty regularization) have been adopted.\n",
    "\n",
    "This is just a shallow implementation: in a proper one we would perform some feature engineering in the naive bayes classifier and model selection in the neural network approach.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger\n",
    "In this part we create a POS tagger for the \"news\" category of the Brown corpus in the NLTK.corpus package. In particular we adopt a windowed approach, that is we set our task as the prediction of the tag of a given word considering a fixed size window around it. Make sure you are able to import all the necessary dependencies as well as providing the proper path of the glove dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8d53dab50a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mglove_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#Load and parse the glove dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.layers import Convolution1D,MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "\n",
    "EMB_DIM = 50\n",
    "UNK_W = np.random.randn(EMB_DIM)\n",
    "\n",
    "#Provide the path to the glove embeddings\n",
    "glove_path = ''\n",
    "#Load and parse the glove dataset\n",
    "f = open(glove_path)\n",
    "embeddings_index = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "#Also download the brown corpus along with the universal tagset\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now parse the tagged news (i.e. a list of lists, with each element being a tuple (word,tag)). We flatten this list, concatenating the inner lists corresponding to the sentences. In doing so we add additional tokens to mark the beginning and the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the tagged news\n",
    "brown_sents = brown.tagged_sents(categories='news', tagset='universal')\n",
    "\n",
    "#Flatten the list, padding with a special token\n",
    "#the start and the end of each sentence\n",
    "flat_brown = []\n",
    "\n",
    "for sent in brown_sents:\n",
    "    flat_brown.append((\"<unk>\", \"UNKTAG\"))\n",
    "    flat_brown.append((\"<unk>\", \"UNKTAG\"))\n",
    "    for w in sent:\n",
    "        flat_brown.append(w)\n",
    "flat_brown.append((\"<unk>\",\"UNKTAG\"))\n",
    "flat_brown.append((\"<unk>\",\"UNKTAG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now specify a window size and create a proper dataset accordingly. I.e. the input x will be the word whose tag we are interested in, and its window. The target y will be the tag of the central word of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Window size, has to be odd\n",
    "win_size = 5\n",
    "assert(win_size%2==1)\n",
    "\n",
    "#Center of the window\n",
    "center = int(np.floor(win_size/2))\n",
    "\n",
    "#Create the n-tuples of window size, each element of the n-tuple is a tuple (word,tag)\n",
    "w_tags = [[flat_brown[i+j] for j in range(0,win_size)] for i in range(0,len(flat_brown)-win_size+1)]\n",
    "#The target tag is the tag of the central word in the windows\n",
    "y_train = [el[center][1] for el in w_tags]\n",
    "#Get rid of the tags for the input x\n",
    "x_train = [[el[0] for el in window] for window in w_tags]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the set of tags and encode them with integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CONJ': 1, 'VERB': 2, 'UNKTAG': 3, '.': 8, 'PRON': 5, 'ADJ': 6, 'ADP': 7, 'NOUN': 9, 'DET': 10, 'ADV': 13, 'PRT': 11, 'X': 12, 'NUM': 4}\n"
     ]
    }
   ],
   "source": [
    "tags = set(y_train)\n",
    "\n",
    "tags_enc = dict()\n",
    "c=1\n",
    "for w in tags:\n",
    "    tags_enc[w] = c\n",
    "    c+=1\n",
    "print(tags_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we update the input data, by replacing each word with its embedding as loaded from the glove embeddings dataset. Note that some words may not be shared between the glove dataset and the brown corpus news dataset (or they might be spelled in a slightly different way): we deal with this in the laziest way, that is replacing all the words that do not appear in the glove corpus with a randomly generated embedding (the same for all such words). The target y is also updated, by replacing the tags with an one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<unk>', '<unk>', 'The', 'Fulton', 'County'], ['<unk>', 'The', 'Fulton', 'County', 'Grand'], ['The', 'Fulton', 'County', 'Grand', 'Jury']]\n",
      "['DET', 'NOUN', 'NOUN']\n",
      "(109798, 5, 50)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x_train_emb = [[\\\n",
    "                (embeddings_index[w.lower()] if w.lower() in embeddings_index.keys() else UNK_W)\\\n",
    "                for w in sent] for sent in x_train]\n",
    "\n",
    "y_train_enc = to_categorical([tags_enc[tag] for tag in y_train])\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "#Shuffle the dataset\n",
    "x_train_emb_shuffled, y_train_enc_shuffled = unison_shuffled_copies(np.array(x_train_emb),np.array(y_train_enc))\n",
    "#Sample input, before replacing words with their embeddings\n",
    "print(x_train[0:3])\n",
    "#Corresponding target\n",
    "print(y_train[0:3])\n",
    "#Input after replacing words with their embeddings, shape is (samples, window size, embedding size)\n",
    "print(x_train_emb_shuffled.shape)\n",
    "#One-hot encoded output\n",
    "print(y_train_enc_shuffled[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset properly formatted we can now separate it into training and test data, then build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108798/108798 [==============================] - 9s 79us/step - loss: 0.6182 - acc: 0.8029\n",
      "Epoch 2/10\n",
      "108798/108798 [==============================] - 8s 76us/step - loss: 0.3668 - acc: 0.8801\n",
      "Epoch 3/10\n",
      "108798/108798 [==============================] - 8s 75us/step - loss: 0.3029 - acc: 0.9005\n",
      "Epoch 4/10\n",
      "108798/108798 [==============================] - 8s 76us/step - loss: 0.2618 - acc: 0.9129\n",
      "Epoch 5/10\n",
      "108798/108798 [==============================] - 8s 75us/step - loss: 0.2336 - acc: 0.9222\n",
      "Epoch 6/10\n",
      "108798/108798 [==============================] - 8s 74us/step - loss: 0.2133 - acc: 0.9280\n",
      "Epoch 7/10\n",
      "108798/108798 [==============================] - 8s 75us/step - loss: 0.1965 - acc: 0.9335\n",
      "Epoch 8/10\n",
      "108798/108798 [==============================] - 8s 74us/step - loss: 0.1816 - acc: 0.9388\n",
      "Epoch 9/10\n",
      "108798/108798 [==============================] - 8s 75us/step - loss: 0.1700 - acc: 0.9418\n",
      "Epoch 10/10\n",
      "108798/108798 [==============================] - 8s 75us/step - loss: 0.1595 - acc: 0.9451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8286859978>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use 1000 sentences as test data\n",
    "x_tr = x_train_emb_shuffled[0:-1000]\n",
    "y_tr = y_train_enc_shuffled[0:-1000]\n",
    "x_ts = x_train_emb_shuffled[-1000:]\n",
    "y_ts = y_train_enc_shuffled[-1000:]\n",
    "\n",
    "#Model definition\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(128, 3, padding='same', input_shape=(5,50),activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(60, activation='relu')),\n",
    "model.add(Dense(14,activation='softmax'))\n",
    "#Compile and fit the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit(x_tr, y_tr, epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can calculate the accuracy, precision, recall and F1 measure for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAINING SET-----\n",
      "Tags mapping {'CONJ': 1, 'VERB': 2, 'UNKTAG': 3, '.': 8, 'PRON': 5, 'ADJ': 6, 'ADP': 7, 'NOUN': 9, 'DET': 10, 'ADV': 13, 'PRT': 11, 'X': 12, 'NUM': 4}\n",
      "\n",
      "Accuracy score per class: 0.9518097759149985\n",
      "\n",
      "Precision per class: [0.99691715 0.94884986 0.9954323  0.98035547 0.99195171 0.79946702\n",
      " 0.98558232 0.99620669 0.95915862 0.99703317 0.80306428 1.\n",
      " 0.7577104 ] \n",
      "Average precision: 0.9393637688507579\n",
      "\n",
      "Recall per class: [0.95779341 0.90400504 0.99967235 0.97852474 0.98246313 0.90456807\n",
      " 0.93367138 0.99991539 0.95148763 0.98263335 0.95809184 0.66304348\n",
      " 0.872142  ] \n",
      "Average recall: 0.9298470621998238\n",
      "\n",
      "F1 score per class: [0.97696375 0.92588476 0.99754782 0.97943925 0.98718462 0.84877635\n",
      " 0.95892482 0.9980576  0.95530772 0.98978089 0.87375483 0.79738562\n",
      " 0.81090909] \n",
      "Average f1 score: 0.9307628550958303\n",
      "\n",
      "Confusion matrix [[ 2587     9     0     0     0     9    21    18    10     0     3     0\n",
      "     44]\n",
      " [    0 12911     2     7     1   331    17     3   667     3    62     0\n",
      "    278]\n",
      " [    0     0  9153     0     0     0     0     0     3     0     0     0\n",
      "      0]\n",
      " [    0     0     0  2096     0     5     0     1    37     2     1     0\n",
      "      0]\n",
      " [    0     9     0     0  2465     0     7     2     1     6     7     0\n",
      "     12]\n",
      " [    0   115     8     6     0  6000     5     0   381     8     4     0\n",
      "    106]\n",
      " [    3    43     0     1     4    68 11416     9    29     9   372     0\n",
      "    273]\n",
      " [    0     0     0     0     0     1     0 11818     0     0     0     0\n",
      "      0]\n",
      " [    1   353    31    22     1   877     9     2 28910     3    31     0\n",
      "    144]\n",
      " [    0    13     0     1    12    48    38     6     7 11090    16     0\n",
      "     55]\n",
      " [    0     4     1     1     0     3    41     1    28     0  2149     0\n",
      "     15]\n",
      " [    0     0     0     1     0     5     0     0    25     0     0    61\n",
      "      0]\n",
      " [    4   150     0     3     2   158    29     3    43     2    31     0\n",
      "   2899]]\n"
     ]
    }
   ],
   "source": [
    "def statistics(x,y):\n",
    "    pred = model.predict(x)\n",
    "    #Compute the confusion matrix\n",
    "    cm = confusion_matrix(y.argmax(1), pred.argmax(1))\n",
    "    print(\"Tags mapping\", tags_enc)\n",
    "    pr = metrics.precision_score(y.argmax(1), pred.argmax(1),average=None)\n",
    "    rc = metrics.recall_score(y.argmax(1), pred.argmax(1),average=None)\n",
    "    f1 = metrics.f1_score(y.argmax(1), pred.argmax(1),average=None)\n",
    "    acc = metrics.accuracy_score(y.argmax(1), pred.argmax(1))\n",
    "    print(\"\\nAccuracy score per class:\", acc)\n",
    "    print(\"\\nPrecision per class:\", pr, \"\\nAverage precision:\",np.average(pr))\n",
    "    print(\"\\nRecall per class:\", rc, \"\\nAverage recall:\",np.average(rc))\n",
    "    print(\"\\nF1 score per class:\", f1, \"\\nAverage f1 score:\",np.average(f1))\n",
    "    print(\"\\nConfusion matrix\", cm)\n",
    "    \n",
    "print(\"-----TRAINING SET-----\")\n",
    "statistics(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TEST SET-----\n",
      "Tags mapping {'CONJ': 1, 'VERB': 2, 'UNKTAG': 3, '.': 8, 'PRON': 5, 'ADJ': 6, 'ADP': 7, 'NOUN': 9, 'DET': 10, 'ADV': 13, 'PRT': 11, 'X': 12, 'NUM': 4}\n",
      "\n",
      "Accuracy score per class: 0.913\n",
      "\n",
      "Precision per class: [1.         0.89380531 0.98876404 0.95652174 1.         0.77333333\n",
      " 0.95       0.97321429 0.93846154 0.98       0.59375    0.56756757] \n",
      "Average precision: 0.8846181515737911\n",
      "\n",
      "Recall per class: [0.8125     0.86324786 1.         0.91666667 1.         0.79452055\n",
      " 0.890625   1.         0.9037037  0.95145631 0.9047619  0.84      ] \n",
      "Average recall: 0.9064568330837464\n",
      "\n",
      "F1 score per class: [0.89655172 0.87826087 0.99435028 0.93617021 1.         0.78378378\n",
      " 0.91935484 0.98642534 0.92075472 0.96551724 0.71698113 0.67741935] \n",
      "Average f1 score: 0.8896307913407985\n",
      "\n",
      "Confusion matrix [[ 13   0   0   0   0   0   0   1   0   0   1   1]\n",
      " [  0 101   0   0   0   4   0   0   8   0   0   4]\n",
      " [  0   0  88   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  22   0   0   0   0   1   0   0   1]\n",
      " [  0   0   0   0  26   0   0   0   0   0   0   0]\n",
      " [  0   4   0   0   0  58   2   0   6   1   1   1]\n",
      " [  0   0   0   0   0   1 114   0   0   0   8   5]\n",
      " [  0   0   0   0   0   0   0 109   0   0   0   0]\n",
      " [  0   6   1   1   0  11   1   0 244   1   1   4]\n",
      " [  0   0   0   0   0   1   2   0   1  98   1   0]\n",
      " [  0   0   0   0   0   0   1   1   0   0  19   0]\n",
      " [  0   2   0   0   0   0   0   1   0   0   1  21]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-----TEST SET-----\")\n",
    "statistics(x_ts, y_ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
